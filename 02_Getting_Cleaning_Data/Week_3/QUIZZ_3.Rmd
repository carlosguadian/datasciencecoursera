---
title: "Getting and Cleaning Data Quiz 3 (JHU) Coursera"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1
-------------
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv

and load the data into R. The code book, describing the variable names is here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf

Create a logical vector that identifies the households on greater than 10 acres who sold more than $10,000 worth of agriculture products. Assign that logical vector to the variable agricultureLogical. Apply the which() function like this to identify the rows of the data frame where the logical vector is TRUE. which(agricultureLogical) 

What are the first 3 values that result?

```{r}
file_url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
download.file(file_url , 'ACS.csv' , method='curl' )

# Read data into data.frame
ACS <- read.csv('ACS.csv')

agricultureLogical <- which(ACS$ACR == 3 & ACS$AGS == 6)
head(agricultureLogical, 3)
```

## Question 2
--------------
Using the jpeg package read in the following picture of your instructor into R

https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg

Use the parameter native=TRUE. What are the 30th and 80th quantiles of the resulting data?

```{r}
library(jpeg)

# Download the file
download.file('https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg' ,'Fjeff.jpg')

# Read the image
Fjeff <- readJPEG('Fjeff.jpg' , native=TRUE)

# Get Sample Quantiles corressponding to given prob
quantile(Fjeff, probs = c(0.3, 0.8) )
```


## Question 3
-------------
Load the Gross Domestic Product data for the 190 ranked countries in this data set:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv

Load the educational data from this data set:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv

Match the data based on the country shortcode. How many of the IDs match? Sort the data frame in descending order by GDP rank. What is the 13th country in the resulting data frame?

Original data sources: http://data.worldbank.org/data-catalog/GDP-ranking-table http://data.worldbank.org/data-catalog/ed-stats


```{r}
library(data.table)
library(dplyr)


# Download data and read FGDP data into data.table
FGDP <- fread('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv', skip=5, nrows = 190, select = c(1, 2, 4, 5), col.names=c("CountryCode", "Rank", "Economy", "Total"))

# Download data and read FGDP data into data.table
FEDSTATS_Country <- fread('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv')
     
mergedDT <- merge(FGDP, FEDSTATS_Country, by = 'CountryCode')

# How many of the IDs match?
nrow(mergedDT)

# Sort the data frame in descending order by GDP rank (so United States is last). 
# What is the 13th country in the resulting data frame?
country13 <- (arrange(mergedDT, desc(Rank)))
country13[13, 3]
```


## Question 4
-------------
What is the average GDP ranking for the "High income: OECD" and "High income: nonOECD" group?

```{r}
# "High income: OECD" 
mergedDT[`Income Group` == "High income: OECD", lapply(.SD, mean), .SDcols = c("Rank"), by = "Income Group"]


# "High income: nonOECD"
mergedDT[`Income Group` == "High income: nonOECD", lapply(.SD, mean), .SDcols = c("Rank"), by = "Income Group"]
```

## Question 5
-------------
Cut the GDP ranking into 5 separate quantile groups. Make a table versus Income.Group. How many countries are Lower middle income but among the 38 nations with highest GDP?

```{r}
# install.packages('dplyr')
library('dplyr')

breaks <- quantile(mergedDT[, Rank], probs = seq(0, 1, 0.2), na.rm = TRUE)
mergedDT$quantileGDP <- cut(mergedDT[, Rank], breaks = breaks)
mergedDT[`Income Group` == "Lower middle income", .N, by = c("Income Group", "quantileGDP")]
```